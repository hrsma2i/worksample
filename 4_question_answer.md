```math
\begin{eqnarray}
4a &=& ((a+a)+a)+a \\
&=& (a+a)+(a+a)
\end{eqnarray}
```

最小二乗法についてのご質問に回答させていただきます。

>・なぜ2乗しているのか

最小二乗法の目的は、観測値（training dataの正解label）に最も近い予測値を出力するmodelのparameterを決めることでした。なので、損失関数（観測値と予測値がどれくらいはなれているかを測る関数）を最小化することになります。  

なぜ2乗しているか、を理解する上で、2乗しなかったらどうなるか、を考えます。

以下の2つのtraining dataにfitするmodelを考えます。

(input, label): (1,0),(0,1)

線形回帰だとすると、損失関数は以下のようになります。
1. $$y=x$$のとき
```math
{0 - (1*1+0)} + {1 - (1*0+0)}
= -1 + 1 = 0
```

2. y=-x+1のとき

```math```
{0 - (-1*1+1)} + {1 - (-1*0+1)}
= 0 + 0 = 0

```

どちらのparamterでも、損失関数の値は同じなので、どちらも同じくらい予測できているmodelということになってしまいます。

しかし、
1. のとき、それぞれの誤差は1, -1
2. のとき、それぞれの誤差は0, 0
であり、2.の方が良いmodelだということは明らかです。

なぜ、このようなことが起きるかというと、1.では、最後の誤差の足し合わせで、誤差どうしが正負で相殺されてしまっているからです。

そこで、各誤差を2乗することにより、
1. y=xのとき
```math
(-1)^2 + 1^2 = 1 + 1 = 2
```

2. y=-x+1のとき

```math```
0^2 + 0^2 = 0
```

となり、1.より2.の方が良いmodelである、と正当に評価できます。なお、損失関数は観測値と予測値がどれくらい離れているかを表せればいいので、負の誤差を正としてカウントしても大丈夫なのです。

最終的な答えは、**2乗しないと、誤差の正負で相殺が起き、損失関数がmodelの良し悪しを適切に測ることができなくなるから**です。

また、今回のように、何かが**なぜあるのかわからない**時は、それが**無い場合に何が起きるか**を考えてみることが、新しい概念を理解するための1つのテクニックです。

---

>・なぜ最後に,2で割っているのか

損失関数をコンピュータで最小化させるには、勾配降下法というアルゴリズムを使うことが多いです。

このアルゴリズムを使う際に、損失関数の微分係数を扱う場面がでてきます。（詳しくは、勾配降下法のところで学んでください。）

モデルをy=ax+bとした場合、損失関数のparameter aでの微分係数は、
```math
\fraq{\partial}{\partial a} \sum_{i} \{\hat{y}\}
```
